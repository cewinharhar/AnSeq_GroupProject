---
title: 'AnSeq_groupProject'
output: pdf_document
---

-   [Introduction]

    -   [Data]

-   [Exploratory Data Analysis (EDA)](#part-1---exploratory-data-analysis-eda)

    -   [Visualization]
    -   [Discussion](#discussion-2)

-   [Indicators]

-   [Simple Model]

    -   [Discussion](#discussion-2)
    -   [Fitting](#fitting-1)

-   [Exponential Smoothing]

    -   [Discussion](#discussion-2)
    -   [Fitting](#fitting-1)
    -   [Analysis](#analysis-1)
    -   [Comparison with simple model]

-   [ETS AND AUTO-ARIMA]

    -   [Fitting](#fitting-1)
    -   [Analysis](#analysis-1)
    -   [Discussion](#discussion-2)
    -   [Comparison with other models]

-   [Conclusion]

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(message = FALSE)

#needed libraries
library(Mcomp)
library(ggplot2)
library()
library(gridExtra)
```

# Introduction (Kevin)

The goal of this project is to sharpen our time series forcasting skills as well as the interpretation of the generated predictions.

## Data

We use the data called n1490 which are datapoints for amazon shipments between the years 1990 to 1996

```{r}
subset(M3, "monthly")["N1483"] %>% str()

data <- subset(M3, "monthly")["N1483"]

autoplot(data$N1483)

```

# Exploratory Data Analysis (EDA) {#part-1---exploratory-data-analysis-eda} (This)

## Visualization

```{r}
#use variable for future tasks
df.Train <- data$N1483$x
df.Test <- data$N1483$xx
```


```{r}
ggsubseriesplot(df.Train) +
  ggtitle("Seasonal plot: Monthly shipments")

ggseasonplot(df.Train, year.labels=TRUE, year.labels.left=TRUE) +
  ggtitle("Seasonal plot: Monthly shipments")

ggAcf(df.Train) 

gglagplot(df.Train)

```

## Discussion

The following section outlines the selected dataset in an explanatory manner to provide a better understanding of the underlying data and to discover patterns and anomalies in the chosen time series.

The chosen dataset consists of 126 entries containing shipment data from 1985 to 1995 with a monthly frequency. As being part of the M3 competition data set, the given time series is already split up in a training and testing set containing 108 and 18 entries respectively.

As represented in the chart N1483, the chosen time series indicates a strong trend as the "amount" of shipments are growing constantly from 1985 to 1995. Furthermore, the graph implies that the time series has no seasonality. The mentioned constant trend and lack of seasonality can also be observed in appendix x and y. Appendix x clearly depicts that there is a constant growing trend in each month over the years in the given time series. In addition, this constant increasing trend over the months underlines that there is no seasonality in sequential data. The same observation can be made in appendix y, which clearly shows that there is now no seasonality as the plotted yearly shipments have no common peaks or lows. Instead the yearly lines tend to increase during the year. This growth over the year leads to the gaps between the line graphs, which represent the shipment over a year. Those described features emphasize the fact that the time series has an increasing trend and no seasonality.

Cycling trend ? -- Meiner Meinung nach nicht

The correlogram as well as the seasonal lag plot in appendix x and z, strengthen the assumption of an increasing trend and a missing seasonality. The autocorrelogram (ACF) in appendix x manifest that the autocorrelations for smaller lags are large and positive, which is typically for trendy data. ACF decreases over time - RÃ¼ckschlÃ¼sse besprechen mit Kevin

GGlagplot mit kevin besprechen alle nahe zusammen jedoch eine Verschiebung zu sehen

# Indicators (Kevin)

USE *MAE* BECAUSE OF UNITS

# Simple Model (This)

## Discussion

blabla

## Fitting

```{r}

```

# Exponential Smoothing (Kevin)

## Discussion {#discussion-2}

For the different methods of the exponential smoothing, one may choose the one which fits best to the available data. 
In our case, due to the seasonal behaviour, the positive trend and the expected stagnation (which is normal for shipments in a company due to various reasons) we choose the holt-Winters damped multiplicative model but for a better overview we fitted all holt-winters model. 

## Analysis

```{r}

df.hwAdd <- hw(df.Train, seasonal = "additive", h = length(df.Test))
df.hwAddDamped <- hw(df.Train, seasonal = "additive", damped = TRUE, h = length(df.Test))
df.hwMulti <- hw(df.Train, seasonal = "multiplicative", h = length(df.Test))
df.hwMultiDamped <- hw(df.Train, seasonal = "multiplicative", damped = TRUE, h = length(df.Test))

#plot

df.hwAddPlot <- autoplot(df.hwAdd, color="AdditiveMethod") +
  labs(x="Time", y="Shipments", color = "Legend") +
  autolayer(df.Test)

df.hwAddDampedPlot <- autoplot(df.hwAddDamped, color="AdditiveMethod") +
  labs(x="Time", y="Shipments", color = "Legend") +
  autolayer(df.Test)

df.hwMultiPlot <- autoplot(df.hwMulti, color="MultiMethod") +
  labs(x="Time", y="Shipments", color = "Legend") +
  autolayer(df.Test)

df.hwMultiDampedPlot <- autoplot(df.hwMultiDamped, color="MultiMethod") +
  labs(x="Time", y="Shipments", color = "Legend") +
  autolayer(df.Test)

grid.arrange(df.hwAddPlot, df.hwAddDampedPlot,  df.hwMultiPlot, df.hwMultiDampedPlot, nrow=4)
```
As can be seen from the above plots, the Damped Holt-Winter's multiplicative Method seems to be the most accurate (by eye) for the predictions. 
This doesnt
Lets evaluate this finding numericaly:

Accuracy of the fitted values for the training data
```{r}

accDf <- rbind(
  accuracy(df.hwAdd$fitted, df.Train),
  accuracy(df.hwAddDamped$fitted, df.Train),
  accuracy(df.hwMulti$fitted, df.Train),
  accuracy(df.hwMultiDamped$fitted, df.Train)
) 
rownames(accDf) <- c("hw Additive", "hw damped Additive", "hw Multiplicative", "hw damped Multiplicative")

print(accDf)

```

It can be seen that the multiplicative Method shows the best overall results for the fitting. 

But how are the forecasting results for these models. For this approach the accuracy of the models mean prediction with the test data is calculated:

```{r}

accDf <- rbind(
  accuracy(df.hwAdd$mean, df.Test),
  accuracy(df.hwAddDamped$mean, df.Test),
  accuracy(df.hwMulti$mean, df.Test),
  accuracy(df.hwMultiDamped$mean, df.Test)
) 
rownames(accDf) <- c("hw Additive", "hw damped Additive", "hw Multiplicative", "hw damped Multiplicative")

print(accDf)
```

For the forcasting the damped multiplicative model shows the best RMSE with 1147.369. Nevertheless, the prediction interval is higher than the undamped model and should therefore be considered. 
The reason why the damped multiplicative model outperformed for forecasting can be explained as followed: 
The data which are handled are shipments over the years. Although the increase in shipemnts over the years seems linear, a company may come to stagnation for various reasons (political, economical, logistical). Therefore with the usage of the damped model this stagnation is considered which can be seen for the $\phi$ parameter in the following block. 
```{r}
summary(df.hwMultiDamped$model)
```
The $\phi$ Parameter has the value 0.98 which shows the change of the slope over time. 

Here the original and fitted values from the (damped) multiplicative model.  
```{r}

original <- ts(c(df.Train,df.Test), start=start(df.Train), frequency=frequency(df.Train))

grid.arrange(
  autoplot(original, series = "original") +
  autolayer(df.hwMultiDamped$fitted, series = "fitted") +
  autolayer(df.hwMultiDamped$mean, series = "predicted") + ggtitle("Damped Multiplicative"),
  
autoplot(original, series = "original") +
  autolayer(df.hwMulti$fitted, series = "fitted") +
  autolayer(df.hwMulti$mean, series = "predicted")  + ggtitle("Multiplicative")
)

```

Check the residuals
```{r}

checkresiduals(df.hwMultiDamped)
```


Cross validation
```{r}

```




## Comparison with simple model

-   problem of linear approach: prediciting far into future will get high uncertainty


# ETS AND AUTO-ARIMA (Kevin)

## Fitting {#fitting-1}

```{r}

```

## Analysis {#analysis-1}

```{r}

```

## Discussion

blabla

## Comparison with other models

```{r}

```

# Conclusion (This)

blabla
