---
title: 'AnSeq_groupProject'
output: pdf_document
---

-   [Introduction]

    -   [Data]

-   [Exploratory Data Analysis (EDA)](#part-1---exploratory-data-analysis-eda)

    -   [Visualization]
    -   [Discussion](#discussion-2)

-   [Indicators]

-   [Simple Model]

    -   [Discussion](#discussion-2)
    -   [Fitting](#fitting-1)

-   [Exponential Smoothing]

    -   [Discussion](#discussion-2)
    -   [Fitting](#fitting-1)
    -   [Analysis](#analysis-1)
    -   [Comparison with simple model]

-   [ETS AND AUTO-ARIMA]

    -   [Fitting](#fitting-1)
    -   [Analysis](#analysis-1)
    -   [Discussion](#discussion-2)
    -   [Comparison with other models]

-   [Conclusion]

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(message = FALSE)

#needed libraries
library(Mcomp)
library(ggplot2)
library(gridExtra)
```

# Introduction (Kevin)

The goal of this project is to sharpen our time series forcasting skills as well as the interpretation of the generated predictions.

## Data

We use the data called n1490 which are datapoints for amazon shipments between the years 1990 to 1996

```{r}
subset(M3, "monthly")["N1483"] %>% str()

data <- subset(M3, "monthly")["N1483"]

autoplot(data$N1483)

```

# Exploratory Data Analysis (EDA) {#part-1---exploratory-data-analysis-eda} (This)

## Visualization

```{r}
#use variable for future tasks
df.Train <- data$N1483$x
df.Test <- data$N1483$xx
df.Test_sub <- window(data$N1483$xx , end=1995)
autoplot(df.Test_sub)
```

```{r}
ggsubseriesplot(df.Train) +
  ggtitle("Seasonal plot: Monthly shipments")

ggseasonplot(df.Train, year.labels=TRUE, year.labels.left=TRUE) +
  ggtitle("Seasonal plot: Monthly shipments")

ggAcf(df.Train) 

gglagplot(df.Train)

```

## Discussion

The following section outlines the selected dataset in an explanatory manner to provide a better understanding of the underlying data and to discover patterns and anomalies in the chosen time series.

The chosen dataset consists of 126 entries containing shipment data from 1985 to 1995 with a monthly frequency. As being part of the M3 competition data set, the given time series is already split up in a training and testing set containing 108 and 18 entries respectively.

As represented in the chart N1483, the chosen time series indicates a strong trend as the "amount" of shipments are growing constantly from 1985 to 1995. Furthermore, the graph implies that the time series has no seasonality. The mentioned constant trend and lack of seasonality can also be observed in appendix x and y. Appendix x clearly depicts that there is a constant growing trend in each month over the years in the given time series. In addition, this constant increasing trend over the months underlines that there is no seasonality in sequential data. The same observation can be made in appendix y, which clearly shows that there is now no seasonality as the plotted yearly shipments have no common peaks or lows. Instead the yearly lines tend to increase during the year. This growth over the year leads to the gaps between the line graphs, which represent the shipment over a year. Those described features emphasize the fact that the time series has an increasing trend and no seasonality.

Cycling trend ? -- Meiner Meinung nach nicht

The correlogram as well as the seasonal lag plot in appendix x and z, strengthen the assumption of an increasing trend and a missing seasonality. The autocorrelogram (ACF) in appendix x manifest that the autocorrelations for smaller lags are large and positive, which is typically for trendy data. ACF decreases over time - Rückschlüsse besprechen mit Kevin

GGlagplot mit kevin besprechen alle nahe zusammen jedoch eine Verschiebung zu sehen

# Indicators (Kevin)

USE *MAE* BECAUSE OF UNITS

# Simple Model (This)

## Discussion

As described in the explorative analysis, the time series contains a upwards trend as well as no seasonality. Due to these characteristics of the time series, the average method model can be neglected preliminary.

Based on the characteristics of the time series, the expectation is that the random walk with drift forecasts the best compared to other simple models as this method forecasts based on the last available observation with the added average of the time series. This characteristic more likely describes the increasing trend in the time series.

The following table illustrates the performance of a the naïve and random walk with drift method based on the chosen indicators. The table shows that MAE and MAPE of the seasonally naive are quite high. Whereas the random walk method with drift and the naive model have the lowest indicators.(MAE und was noch?)

As visualized in appendix y the residuals of the seasonal naïve forecasting method remain rather high. The high residuals of the seasonal naïve model are expected as the chosen time series has no seasonality. Eventhough the seasonal naïve model has a p-value of 0.6792 with a Q\* of 7.4828 and therefore is not rejected by ull hypothesis of the Ljung-Box test, the model is no suitable as the residuals are too high.

Comparing the indicators of the naïve and the random walk method, we see that the random walk method is slightly better in training accuracy. However, the naïve method performs better on test data than the random walk with drift. This can be explained by the fact that the slope of the trend decreases slightly towards the end of the time series.

The Indicators of the naïve and the random walk method are clearly better than the seasonal naïve methode. Although for both models, the null hypothesis of the Ljung-Box test has to be rejected as the p-value is 0.006729 with a Q\* of 24.348 and 0.003784 with a Q\* of 24.348 respectively. Thus, both forecasts are biased. Consequently, none of the simple models is suitable for the given time series.

Notes:

Prognose über kleineren Zeitraum für suitable model\`? wie kann ich das machen?

Random walk with drift jedes mal ausschreiben?

A good way to choose the best forecasting model is to nd the model with the smallest RMSE computed using time series cross-validation. Tool box Slide 50

Training und Test liefert unterschiedliche Resultate -- consider Cross validation

## Fitting

```{r}
naive_shipment <- naive(df.Train, h=18)
autoplot(naive_shipment)
snaive_shipment <- snaive(df.Train, h=18)
autoplot(snaive_shipment)
rwf_shipment <- rwf(df.Train, drift = TRUE, h=18)
autoplot(rwf_shipment)

autoplot(rwf_shipment)+autolayer(fitted(rwf_shipment), series = "fitted")
summary(rwf_shipment)


checkresiduals(naive_shipment)
checkresiduals(snaive_shipment)
checkresiduals(rwf_shipment)


```

```{r}
accuracy(naive_shipment, df.Test)
accuracy(snaive_shipment, df.Test)
accuracy(rwf_shipment, df.Test)


#test mit kurzer prediction
#accuracy(naive_shipment, df.Test_sub)
#accuracy(snaive_shipment, df.Test_sub)
#accuracy(rwf_shipment, df.Test_sub)
```

# Exponential Smoothing (Kevin)

## Discussion {#discussion-2}

For the different methods of the exponential smoothing, one may choose the one which fits best to the available data. In our case, due to the seasonal behaviour, the positive trend and the expected stagnation (which is normal for shipments in a company due to various reasons) we choose the holt-Winters damped multiplicative model but for a better overview we fitted all holt-winters model.

## Analysis

```{r}

df.hwAdd <- hw(df.Train, seasonal = "additive", h = length(df.Test))
df.hwAddDamped <- hw(df.Train, seasonal = "additive", damped = TRUE, h = length(df.Test))
df.hwMulti <- hw(df.Train, seasonal = "multiplicative", h = length(df.Test))
df.hwMultiDamped <- hw(df.Train, seasonal = "multiplicative", damped = TRUE, h = length(df.Test))

#plot

df.hwAddPlot <- autoplot(df.hwAdd, color="AdditiveMethod") +
  labs(x="Time", y="Shipments", color = "Legend") +
  autolayer(df.Test)

df.hwAddDampedPlot <- autoplot(df.hwAddDamped, color="AdditiveMethod") +
  labs(x="Time", y="Shipments", color = "Legend") +
  autolayer(df.Test)

df.hwMultiPlot <- autoplot(df.hwMulti, color="MultiMethod") +
  labs(x="Time", y="Shipments", color = "Legend") +
  autolayer(df.Test)

df.hwMultiDampedPlot <- autoplot(df.hwMultiDamped, color="MultiMethod") +
  labs(x="Time", y="Shipments", color = "Legend") +
  autolayer(df.Test)

grid.arrange(df.hwAddPlot, df.hwAddDampedPlot,  df.hwMultiPlot, df.hwMultiDampedPlot, nrow=4)
```

As can be seen from the above plots, the Damped Holt-Winter's multiplicative Method seems to show a similar forecast to the test set. Before evaluating the accuracy we want to check the residuals.

#### Residuals

```{r}
checkresiduals(df.hwMultiDamped)
```

The Ljung-Box test shows a p-value \<\< 0.05 and therefore the NUll-Hypothesis is rejected. The residuals might be auto-correlated.

From the ACF a pattern is difficult to detect but s significant-autocorrelation is shown at lag 4 and 11. R12 is small and indicates no seasonality.

Accuracy of the fitted values for the training data

```{r}

accDf <- rbind(
  accuracy(df.hwAdd$fitted, df.Train),
  accuracy(df.hwAddDamped$fitted, df.Train),
  accuracy(df.hwMulti$fitted, df.Train),
  accuracy(df.hwMultiDamped$fitted, df.Train)
) 
rownames(accDf) <- c("hw Additive", "hw damped Additive", "hw Multiplicative", "hw damped Multiplicative")

print(accDf)

```

It can be seen that the multiplicative Method shows the best results for the fitting in this scenario.

But how are the forecasting results for these models. For this approach the accuracy of the models mean prediction with the test data is calculated:

```{r}

accDf <- rbind(
  accuracy(df.hwAdd$mean, df.Test),
  accuracy(df.hwAddDamped$mean, df.Test),
  accuracy(df.hwMulti$mean, df.Test),
  accuracy(df.hwMultiDamped$mean, df.Test)
) 
rownames(accDf) <- c("hw Additive", "hw damped Additive", "hw Multiplicative", "hw damped Multiplicative")

print(accDf)
```

For the forcasting the damped multiplicative model shows the best RMSE with 1147.369. Nevertheless, the prediction interval is higher than the undamped model and should therefore be considered. The reason why the damped multiplicative model outperformed for forecasting can be explained as followed: The data which are handled are shipments over the years. Although the increase in shipemnts over the years seems linear, a company may come to stagnation for various reasons (political, economical, logistical). Therefore with the usage of the damped model this stagnation is considered which can be seen for the $\phi$ parameter in the following block.

```{r}
summary(df.hwMultiDamped$model)
```

The $\phi$ Parameter has the value 0.98 which shows the change of the slope over time.

Here the original and fitted values from the damped multiplicative model.

```{r}

original <- ts(c(df.Train,df.Test), start=start(df.Train), frequency=frequency(df.Train))

autoplot(original, series = "original") +
autolayer(df.hwMultiDamped$fitted, series = "fitted") +
autolayer(df.hwMultiDamped$mean, series = "predicted") + ggtitle("Damped Multiplicative")


```

## Comparison with simple model (THIS)

```{r}

accDf <- rbind(
  accuracy(df.lin$mean, df.Test),  #da chunt dis beste model ine
  accuracy(df.hwAddDamped$mean, df.Test)
) 
rownames(accDf) <- c("XXX", "hw damped Multiplicative")

print(accDf)

```

We can see that the damped holts-Winter method outperformes the linear model. This

Furthermore the predicition interval of the linear model gets very high which makes it hard to predict far into the future.

# ETS AND AUTO-ARIMA (Kevin)

## Fitting {#fitting-1}

#### ETS

```{r}

df.ets <- ets(df.Train)
summary(df.ets)

autoplot(df.ets)

checkresiduals(df.ets)

```

The ets approach defined holts damped linear trend method to be the most suitable candidate.

The p-value is \> 0.5 and the AFC doesn't show any correlation which is good. The Residual spread looks normal with no clear bias visible.

#### auto.arima

The data doesn't seem to have changing variance and therefore the log10 transformation is not used.

```{r}

df.autoarima <- auto.arima(df.Train)
summary(df.autoarima)

checkresiduals(df.autoarima)

```

The auto.arima approach defined ARIMA(0,1,1) with drift to be the best candidate. This means its a model with no autoregression, using the first derivative and considering seasonality. Additionaly a constant is used which is shows as drift.

The p-value is \>\> 0.5 and the AFC doesnt show any correlation which is good. The Residual spread looks normal with no clear bias visible.

## Perfomance analysis {#analysis-1}

### Test set

```{r}

df.etsFor <- df.ets %>% forecast(h=18) 
df.autoarimaFor <- df.autoarima %>% forecast(h=18) 

accDf <- rbind(
  accuracy(df.etsFor$mean, df.Test),
  accuracy(df.autoarimaFor$mean, df.Test)
) 
rownames(accDf) <- c("ets", "autoarima")

print(accDf)

```

As we can see from the forecasting accuracy (MAE) on the test set, the autoarima model shows a better result than the ets approach by almost 400 error units.

Nevertheless, a better approach to choose the forecasting model is to run a crossvalidation with the original data containing train and test set.

### CV on train & test set

```{r}
#the result of ets gave an additive error, damped additive trend and no season holts approach (holt)
holtError <- tsCV(original, holt, drift = TRUE, h=1, damped = TRUE)

#results of auto.arima gave ARIMA(0,1,1) with constant
arimaFun <- function(x, h){forecast(Arima(x, order=c(0,1,1)), h=h, include.constant = TRUE)} #helper function
arimaError <- tsCV(original, arimaFun, h=1)

print("Holt CV RMSE")
print(sqrt(mean(holtError^2, na.rm = TRUE)))
print("Arima CV RMSE")
print(sqrt(mean(arimaError^2, na.rm = TRUE)))

```

For the Cross-Validation with Train and Test set we see that the RMSE of the holts linear trend is smaller than the Arima(0,1,1) approach. Nevertheless, with a difference of only 55.02 the decision for the better model is still difficult.

```{r}
autoplot(original, series = "original") +
autolayer(df.ets$fitted, series = "fitted") +
autolayer(df.etsFor$mean, series = "predicted") + ggtitle("ETS approach")

autoplot(original, series = "original") +
autolayer(df.autoarima$fitted, series = "fitted") +
autolayer(df.autoarimaFor$mean, series = "predicted") + ggtitle("Auto Arima approach")

```

## Comparison with other models

```{r}



```

# Conclusion (This)

blabla
